{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the cities were in average on the 7 days the probability to have rain is the lowest\n",
    "\n",
    "dataset=pd.read_csv('villes - Projection meteo.csv')\n",
    "\n",
    "dataset_annexe = dataset.groupby('city_name')['prob_precipitation']\\\n",
    "                .mean()\\\n",
    "                .sort_values(ascending=True)\\\n",
    "                .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aigues Mortes', 'Saintes Maries de la mer', 'Nimes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_cities = dataset_annexe.iloc[0:3, :]['city_name']\n",
    "top_cities_name = list(top_cities)\n",
    "top_cities_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Scrapy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 09:15:47 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: scrapybot)\n",
      "2021-09-20 09:15:47 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-5.4.109+-x86_64-with-glibc2.10\n",
      "2021-09-20 09:15:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-09-20 09:15:47 [scrapy.extensions.telnet] INFO: Telnet Password: 025241abf707ac05\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-09-20 09:15:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-09-20 09:15:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-09-20 09:15:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-09-20 09:15:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-09-20 09:15:47 [scrapy.extensions.telnet] INFO: Telnet Password: 1faefbcb9ca671c2\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-09-20 09:15:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-09-20 09:15:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-09-20 09:15:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2021-09-20 09:15:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-09-20 09:15:47 [scrapy.extensions.telnet] INFO: Telnet Password: 1164ad33d14a301b\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-09-20 09:15:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-09-20 09:15:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-09-20 09:15:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-09-20 09:15:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025\n",
      "2021-09-20 09:15:53 [root] INFO: No next page. Terminating crawling process.\n",
      "2021-09-20 09:15:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-09-20 09:15:53 [scrapy.extensions.feedexport] INFO: Stored json feed (97 items) in: res/_hotel_booking.json\n",
      "2021-09-20 09:15:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 7340,\n",
      " 'downloader/request_count': 5,\n",
      " 'downloader/request_method_count/GET': 5,\n",
      " 'downloader/response_bytes': 790500,\n",
      " 'downloader/response_count': 5,\n",
      " 'downloader/response_status_count/200': 5,\n",
      " 'elapsed_time_seconds': 5.623249,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 9, 20, 9, 15, 53, 63672),\n",
      " 'httpcompression/response_bytes': 4205772,\n",
      " 'httpcompression/response_count': 5,\n",
      " 'item_scraped_count': 97,\n",
      " 'log_count/INFO': 30,\n",
      " 'memusage/max': 119255040,\n",
      " 'memusage/startup': 119255040,\n",
      " 'request_depth_max': 4,\n",
      " 'response_received_count': 5,\n",
      " 'scheduler/dequeued': 5,\n",
      " 'scheduler/dequeued/memory': 5,\n",
      " 'scheduler/enqueued': 5,\n",
      " 'scheduler/enqueued/memory': 5,\n",
      " 'start_time': datetime.datetime(2021, 9, 20, 9, 15, 47, 440423)}\n",
      "2021-09-20 09:15:53 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2021-09-20 09:15:55 [root] INFO: No next page. Terminating crawling process.\n",
      "2021-09-20 09:15:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-09-20 09:15:55 [scrapy.extensions.feedexport] INFO: Stored json feed (128 items) in: res/_hotel_booking.json\n",
      "2021-09-20 09:15:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 10816,\n",
      " 'downloader/request_count': 7,\n",
      " 'downloader/request_method_count/GET': 7,\n",
      " 'downloader/response_bytes': 1124303,\n",
      " 'downloader/response_count': 7,\n",
      " 'downloader/response_status_count/200': 7,\n",
      " 'elapsed_time_seconds': 8.189014,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 9, 20, 9, 15, 55, 650604),\n",
      " 'httpcompression/response_bytes': 6181273,\n",
      " 'httpcompression/response_count': 7,\n",
      " 'item_scraped_count': 128,\n",
      " 'log_count/INFO': 26,\n",
      " 'memusage/max': 119496704,\n",
      " 'memusage/startup': 119496704,\n",
      " 'request_depth_max': 6,\n",
      " 'response_received_count': 7,\n",
      " 'scheduler/dequeued': 7,\n",
      " 'scheduler/dequeued/memory': 7,\n",
      " 'scheduler/enqueued': 7,\n",
      " 'scheduler/enqueued/memory': 7,\n",
      " 'start_time': datetime.datetime(2021, 9, 20, 9, 15, 47, 461590)}\n",
      "2021-09-20 09:15:55 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2021-09-20 09:15:58 [root] INFO: No next page. Terminating crawling process.\n",
      "2021-09-20 09:15:58 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-09-20 09:15:58 [scrapy.extensions.feedexport] INFO: Stored json feed (214 items) in: res/_hotel_booking.json\n",
      "2021-09-20 09:15:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 15744,\n",
      " 'downloader/request_count': 10,\n",
      " 'downloader/request_method_count/GET': 10,\n",
      " 'downloader/response_bytes': 1612678,\n",
      " 'downloader/response_count': 10,\n",
      " 'downloader/response_status_count/200': 10,\n",
      " 'elapsed_time_seconds': 11.355415,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 9, 20, 9, 15, 58, 834255),\n",
      " 'httpcompression/response_bytes': 8810058,\n",
      " 'httpcompression/response_count': 10,\n",
      " 'item_scraped_count': 214,\n",
      " 'log_count/INFO': 22,\n",
      " 'memusage/max': 119496704,\n",
      " 'memusage/startup': 119496704,\n",
      " 'request_depth_max': 9,\n",
      " 'response_received_count': 10,\n",
      " 'scheduler/dequeued': 10,\n",
      " 'scheduler/dequeued/memory': 10,\n",
      " 'scheduler/enqueued': 10,\n",
      " 'scheduler/enqueued/memory': 10,\n",
      " 'start_time': datetime.datetime(2021, 9, 20, 9, 15, 47, 478840)}\n",
      "2021-09-20 09:15:58 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "#Creation of the class BookingSpider(scrapy.Spider)\n",
    "\n",
    "destination_name = top_cities_name #Name to be replaced with the city to be studied\n",
    "\n",
    "class BookingSpider1(scrapy.Spider):\n",
    "    name = \"hotels\"\n",
    "\n",
    "    start_urls = [\"https://www.booking.com/index.fr.html\"]\n",
    "\n",
    "# Parse function for choosing the city\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to choose\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata = {'ss': destination_name[0]},\n",
    "            callback = self.after_search\n",
    "        )\n",
    "\n",
    "    def after_search(self, response):\n",
    "\n",
    "\n",
    "\n",
    "        for hotel in response.css('.sr_item'):\n",
    "            yield {\n",
    "                'city': destination_name[0],\n",
    "                'name': hotel.css('.sr-hotel__name::text').get(),\n",
    "                'url': \"https://www.booking.com\" + hotel.css('.sr-hotel__title a').attrib[\"href\"],\n",
    "                'gps': hotel.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
    "                'score': hotel.css('.bui-review-score__badge::text').get(),\n",
    "                'description': hotel.css('.hotel_desc::text').get()\n",
    "\n",
    "            }\n",
    "\n",
    "        # Select the NEXT button and store it in next_page\n",
    "\n",
    "        try:\n",
    "                next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "                logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            yield response.follow(next_page, callback = self.after_search)\n",
    "\n",
    "class BookingSpider2(scrapy.Spider):\n",
    "        name = \"hotels\"\n",
    "\n",
    "        start_urls = ['https://www.booking.com/index.fr.html']\n",
    "\n",
    "    # Parse function for choosing the city\n",
    "        def parse(self, response):\n",
    "            # FormRequest used to choose\n",
    "            return scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata = {'ss': destination_name[1]},\n",
    "                callback = self.after_search\n",
    "            )\n",
    "\n",
    "        def after_search(self, response):\n",
    "\n",
    "\n",
    "\n",
    "            for hotel in response.css('.sr_item'):\n",
    "                yield {\n",
    "                    'city': destination_name[1],\n",
    "                    'name': hotel.css('.sr-hotel__name::text').get(),\n",
    "                    'url': \"https://www.booking.com\" + hotel.css('.sr-hotel__title a').attrib[\"href\"],\n",
    "                    'gps': hotel.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
    "                    'score': hotel.css('.bui-review-score__badge::text').get(),\n",
    "                    'description': hotel.css('.hotel_desc::text').get()\n",
    "\n",
    "                }\n",
    "\n",
    "            # Select the NEXT button and store it in next_page\n",
    "\n",
    "            try:\n",
    "                    next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "            except KeyError:\n",
    "                    logging.info('No next page. Terminating crawling process.')\n",
    "            else:\n",
    "                yield response.follow(next_page, callback = self.after_search)\n",
    "\n",
    "class BookingSpider3(scrapy.Spider):\n",
    "        name = \"hotels\"\n",
    "\n",
    "        start_urls = ['https://www.booking.com/index.fr.html']\n",
    "\n",
    "    # Parse function for choosing the city\n",
    "        def parse(self, response):\n",
    "            # FormRequest used to choose\n",
    "            return scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata = {'ss': destination_name[2]},\n",
    "                callback = self.after_search\n",
    "            )\n",
    "\n",
    "        def after_search(self, response):\n",
    "\n",
    "\n",
    "\n",
    "            for hotel in response.css('.sr_item'):\n",
    "                yield {\n",
    "                    'city': destination_name[2],\n",
    "                    'name': hotel.css('.sr-hotel__name::text').get(),\n",
    "                    'url': \"https://www.booking.com\" + hotel.css('.sr-hotel__title a').attrib[\"href\"],\n",
    "                    'gps': hotel.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
    "                    'score': hotel.css('.bui-review-score__badge::text').get(),\n",
    "                    'description': hotel.css('.hotel_desc::text').get()\n",
    "\n",
    "                }\n",
    "\n",
    "            # Select the NEXT button and store it in next_page\n",
    "\n",
    "            try:\n",
    "                    next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "            except KeyError:\n",
    "                    logging.info('No next page. Terminating crawling process.')\n",
    "            else:\n",
    "                yield response.follow(next_page, callback = self.after_search)\n",
    "               \n",
    "                \n",
    "# Name of the file where the results will be saved\n",
    "filename = \"_hotel_booking.json\"\n",
    "\n",
    "#If file already exists, delete it before crawling\n",
    "if filename in os.listdir('res/'):\n",
    "    os.remove('res/' + filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)', #ne fonctionne pas avec Mozilla ?\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'res/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(BookingSpider1)\n",
    "process.crawl(BookingSpider2)\n",
    "process.crawl(BookingSpider3)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a list containing the names of the files in the directory.\n",
       "\n",
       "path can be specified as either str, bytes, or a path-like object.  If path is bytes,\n",
       "  the filenames returned will also be bytes; in all other circumstances\n",
       "  the filenames returned will be str.\n",
       "If path is None, uses the path='.'.\n",
       "On some platforms, path may also be specified as an open file descriptor;\\\n",
       "  the file descriptor must refer to a directory.\n",
       "  If this functionality is unavailable, using it raises NotImplementedError.\n",
       "\n",
       "The list is in arbitrary order.  It does not include the special\n",
       "entries '.' and '..' even if they are present in the directory.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.listdir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Remove a file (same as unlink()).\n",
       "\n",
       "If dir_fd is not None, it should be a file descriptor open to a directory,\n",
       "  and path should be relative; path will then be relative to that directory.\n",
       "dir_fd may not be implemented on your platform.\n",
       "  If it is unavailable, using it will raise a NotImplementedError.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>city_id</th>\n",
       "      <th>name</th>\n",
       "      <th>hotel_id</th>\n",
       "      <th>url</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>score</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [city_name, city_id, name, hotel_id, url, latitude, longitude, score, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creation of Dataframe for the top 3 cities with details on hotels\n",
    "\n",
    "df_top_cities_hotels = pd.DataFrame(columns = [\"city_name\", \"city_id\", \"name\", \"hotel_id\", \"url\",\n",
    "                                        \"latitude\", \"longitude\", \"score\", \"description\"])\n",
    "df_top_cities_hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7131509dbfda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Reading JSON files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"res/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_hotel_booking.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'hotel_id'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"city_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcity_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1119\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "for city in top_cities_name:\n",
    "    city_name = city\n",
    "    city_id = dataset.loc[dataset[\"city_name\"] == city,\"city_id\"].values[0]\n",
    "    \n",
    "    #Reading JSON files\n",
    "    path = \"res/\" + city +'_hotel_booking.json'\n",
    "    temp = pd.read_json(str(path))\n",
    "    temp = temp.reset_index().rename({'index': 'hotel_id'}, axis = 1)\n",
    "    temp.loc[:, \"city_name\"] = city_name\n",
    "    temp.loc[:, \"city_id\"] = city_id\n",
    "    \n",
    "    #Clean name, desc and link\n",
    "    temp[\"description\"] = temp[\"description\"].apply(lambda x : x.replace('\\n',''))\n",
    "    temp[\"name\"] = temp[\"name\"].apply(lambda x : x.replace('\\n',''))\n",
    "    temp[\"url\"] = temp[\"url\"].apply(lambda x : x.replace('\\n',''))\n",
    "    \n",
    "    #Transform score\n",
    "    temp[\"score\"] = temp[\"score\"].str.replace(',','.')\n",
    "    \n",
    "    \n",
    "    #Get the lat and lon out of GPS information\n",
    "    temp.loc[:, \"gps\"] = temp[\"gps\"].str.split(',')\n",
    "    temp.loc[:, \"latitude\"] = temp[\"gps\"].apply(lambda x : x[1])\n",
    "    temp.loc[:, \"longitude\"] = temp[\"gps\"].apply(lambda x : x[0])\n",
    "    \n",
    "    temp = temp.drop('gps', axis = 1)\n",
    "    temp = temp.drop('city', axis = 1)\n",
    "    \n",
    "    \n",
    "    df_top_cities_hotels = df_top_cities_hotels.append(temp)\n",
    "    \n",
    "df_top_cities_hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2cb2bef44510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"res/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_hotel_booking.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0;31m#print( open(path).read() )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m    \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m    \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'hotel_id'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"city_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcity_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1119\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "for city in top_cities_name:\n",
    "    city_name = city\n",
    "    city_id = dataset.loc[dataset[\"city_name\"] == city,\"city_id\"].values[0]\n",
    "    \n",
    "    #Reading JSON files\n",
    "    path = \"res/\" + city +'_hotel_booking.json'\n",
    "    #print( open(path).read() )\n",
    "    temp = pd.read_json(str(path))\n",
    "    temp = temp.reset_index().rename({'index': 'hotel_id'}, axis = 1)\n",
    "    temp.loc[:, \"city_name\"] = city_name\n",
    "    temp.loc[:, \"city_id\"] = city_id\n",
    "    \n",
    "    #Clean name, desc and link\n",
    "    temp[\"description\"] = temp[\"description\"].apply(lambda x : x.replace('\\n',''))\n",
    "    temp[\"name\"] = temp[\"name\"].apply(lambda x : x.replace('\\n',''))\n",
    "    temp[\"url\"] = temp[\"url\"].apply(lambda x : x.replace('\\n',''))\n",
    "    \n",
    "    #Transform score\n",
    "    temp[\"score\"] = temp[\"score\"].str.replace(',','.')\n",
    "    \n",
    "    \n",
    "    #Get the lat and lon out of GPS information\n",
    "    temp.loc[:, \"gps\"] = temp[\"gps\"].str.split(',')\n",
    "    temp.loc[:, \"latitude\"] = temp[\"gps\"].apply(lambda x : x[1])\n",
    "    temp.loc[:, \"longitude\"] = temp[\"gps\"].apply(lambda x : x[0])\n",
    "    \n",
    "    temp = temp.drop('gps', axis = 1)\n",
    "    temp = temp.drop('city', axis = 1)\n",
    "    \n",
    "    \n",
    "    df_top_cities_hotels = df_top_cities_hotels.append(temp)\n",
    "    \n",
    "df_top_cities_hotels\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Change type  \n",
    "df_top_cities_hotels[\"latitude\"] = df_top_cities_hotels[\"latitude\"].astype(float)\n",
    "df_top_cities_hotels[\"longitude\"] = df_top_cities_hotels[\"longitude\"].astype(float)\n",
    "df_top_cities_hotels[\"score\"] = df_top_cities_hotels[\"score\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsort_long = df_top_cities_hotels.sort_values(by=\"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsort_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change type  \n",
    "df_top_cities_hotels[\"latitude\"] = df_top_cities_hotels[\"latitude\"].astype(float)\n",
    "df_top_cities_hotels[\"longitude\"] = df_top_cities_hotels[\"longitude\"].astype(float)\n",
    "df_top_cities_hotels[\"score\"] = df_top_cities_hotels[\"score\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the weather information in a CSV file\n",
    "df_top_cities_hotels.to_csv('res/3 best cities - hotels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Using cached plotly-5.3.1-py2.py3-none-any.whl (23.9 MB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from plotly) (1.15.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Using cached tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.3.1 tenacity-8.0.1\n"
     ]
    }
   ],
   "source": [
    "#Creating map for the 3 best cities\n",
    "\n",
    "!pip install plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# setting Jedha color palette as default\n",
    "pio.templates[\"jedha\"] = go.layout.Template(\n",
    "    layout_colorway=[\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    ")\n",
    "pio.templates.default = \"jedha\"\n",
    "pio.renderers.default = \"iframe\" # to be replaced by \"iframe\" if working on JULIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>city_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>average_temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nimes</td>\n",
       "      <td>24</td>\n",
       "      <td>43.837425</td>\n",
       "      <td>4.360069</td>\n",
       "      <td>30.927143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>25</td>\n",
       "      <td>43.565822</td>\n",
       "      <td>4.191284</td>\n",
       "      <td>27.864286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Saintes Maries de la mer</td>\n",
       "      <td>26</td>\n",
       "      <td>43.452277</td>\n",
       "      <td>4.428717</td>\n",
       "      <td>26.188571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  city_name  city_id   latitude  longitude  \\\n",
       "0                     Nimes       24  43.837425   4.360069   \n",
       "1             Aigues Mortes       25  43.565822   4.191284   \n",
       "2  Saintes Maries de la mer       26  43.452277   4.428717   \n",
       "\n",
       "   average_temperature  \n",
       "0            30.927143  \n",
       "1            27.864286  \n",
       "2            26.188571  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Creating a new dataset with latitude and longitude infos for the 10 best cities\n",
    "\n",
    "df_top_cities = dataset.drop([\"day_month\", \"weather_desc\", \"prob_precipitation\", \"daily_temperature\"], axis=1)\n",
    "\n",
    "df_top_cities = df_top_cities.drop_duplicates()\n",
    "\n",
    "df_top_cities = df_top_cities.loc[df_top_cities[\"city_name\"]\\\n",
    "                                .isin(top_cities_name)]\\\n",
    "                                .reset_index()\\\n",
    "                                .drop(\"index\", axis = 1)\n",
    "df_top_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_15.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displaying a map of these 3 top cities with their average temperature\n",
    "\n",
    "fig = px.scatter_mapbox(df_top_cities, \n",
    "                        lat = \"latitude\", \n",
    "                        lon = \"longitude\",\n",
    "                        color = \"average_temperature\", \n",
    "                        hover_name = \"city_name\",\n",
    "                        mapbox_style = \"carto-positron\", \n",
    "                        size = \"average_temperature\",\n",
    "                        title = 'Average Temperatures in selected cities')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.write_html(fig, \"average_temperature_in_selected_cities.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   city_name    0 non-null      object \n",
      " 1   city_id      0 non-null      object \n",
      " 2   name         0 non-null      object \n",
      " 3   hotel_id     0 non-null      object \n",
      " 4   url          0 non-null      object \n",
      " 5   latitude     0 non-null      float64\n",
      " 6   longitude    0 non-null      float64\n",
      " 7   score        0 non-null      float64\n",
      " 8   description  0 non-null      object \n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_top_cities_hotels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>city_id</th>\n",
       "      <th>name</th>\n",
       "      <th>hotel_id</th>\n",
       "      <th>url</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>score</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [city_name, city_id, name, hotel_id, url, latitude, longitude, score, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning the non-filled score\n",
    "df_top_cities_hotels_with_score = df_top_cities_hotels.sort_values(by=['score'], ascending=False)\n",
    "#df_top_cities_hotels_with_score = df_top_cities_hotels_with_score.iloc[0:10]\n",
    "\n",
    "df_top_cities_hotels_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_19.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Creating the map\n",
    "fig2 = px.scatter_mapbox(df_top_cities_hotels_with_score, \n",
    "                        lat = \"latitude\",\n",
    "                        lon = \"longitude\",\n",
    "                        color = \"score\",\n",
    "                        size = \"score\",\n",
    "                        hover_name = \"name\",\n",
    "                        mapbox_style = \"carto-positron\",\n",
    "                        title = 'Best hotels in top 3 cities')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.write_html(fig, \"best_hotels_in_top_3_cities.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
